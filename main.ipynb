{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corpus\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CaboCha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 0 1D 0/1 0.516644\n",
      "明日\t名詞,副詞可能,*,*,*,*,明日,アシタ,アシタ\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "* 1 3D 0/1 1.189807\n",
      "暑い\t形容詞,自立,*,*,形容詞・アウオ段,基本形,暑い,アツイ,アツイ\n",
      "し\t助詞,接続助詞,*,*,*,*,し,シ,シ\n",
      "* 2 3D 0/1 2.251458\n",
      "湿度\t名詞,一般,*,*,*,*,湿度,シツド,シツド\n",
      "も\t助詞,係助詞,*,*,*,*,も,モ,モ\n",
      "* 3 8D 0/1 -1.415575\n",
      "高い\t形容詞,自立,*,*,形容詞・アウオ段,基本形,高い,タカイ,タカイ\n",
      "ので\t助詞,接続助詞,*,*,*,*,ので,ノデ,ノデ\n",
      "、\t記号,読点,*,*,*,*,、,、,、\n",
      "* 4 8D 0/1 -1.415575\n",
      "部活\t名詞,一般,*,*,*,*,部活,ブカツ,ブカツ\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "* 5 6D 0/1 2.162031\n",
      "休み\t名詞,一般,*,*,*,*,休み,ヤスミ,ヤスミ\n",
      "に\t助詞,格助詞,一般,*,*,*,に,ニ,ニ\n",
      "* 6 8D 0/1 -1.415575\n",
      "し\t動詞,自立,*,*,サ変・スル,連用形,する,シ,シ\n",
      "て\t助詞,接続助詞,*,*,*,*,て,テ,テ\n",
      "* 7 8D 0/1 -1.415575\n",
      "早め\t名詞,一般,*,*,*,*,早め,ハヤメ,ハヤメ\n",
      "に\t助詞,格助詞,一般,*,*,*,に,ニ,ニ\n",
      "* 8 -1D 0/2 0.000000\n",
      "帰り\t動詞,自立,*,*,五段・ラ行,連用形,帰る,カエリ,カエリ\n",
      "ましょ\t助動詞,*,*,*,特殊・マス,未然ウ接続,ます,マショ,マショ\n",
      "う\t助動詞,*,*,*,不変化型,基本形,う,ウ,ウ\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c = CaboCha.Parser()\n",
    "txt = '明日は暑いし湿度も高いので、部活は休みにして早めに帰りましょう。'\n",
    "#txt = 'そうしたらそのワンちゃんがなんかか喜んじゃって、で、あたしの方に走ってきて、とびついてきちゃってさ。'\n",
    "tree = c.parse(txt)\n",
    "print(tree.toString(CaboCha.FORMAT_LATTICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 0 1 明日\n",
      "1 3 0 1 暑い\n",
      "2 3 0 1 湿度\n",
      "3 8 0 1 高い\n",
      "4 8 0 1 部活\n",
      "5 6 0 1 休み\n",
      "6 8 0 1 し\n",
      "7 8 0 1 早め\n",
      "8 -1 0 2 帰り\n"
     ]
    }
   ],
   "source": [
    "id = 0\n",
    "for i in range(tree.size()):\n",
    "    token = tree.token(i)\n",
    "    if(token.chunk):\n",
    "        tmp = token.chunk\n",
    "        print(id, tmp.link, tmp.head_pos, tmp.func_pos, token.surface)\n",
    "        id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bunsetsu_list(tree):\n",
    "    result = []\n",
    "    chunkindex = 0\n",
    "    for i in range(tree.size()):\n",
    "        token = tree.token(i)\n",
    "        if(token.chunk is not None):\n",
    "            features = token.feature.split(',')\n",
    "            result.append({'lemma':features[-3], 'link':token.chunk.link, 'index':chunkindex})\n",
    "            chunkindex += 1\n",
    "    return result\n",
    "\n",
    "def search_candidate(known_word_index, blist):\n",
    "    result = set()\n",
    "    for b in blist:\n",
    "        if((b['link'] == known_word_index) or (b['index'] == blist[known_word_index]['link'])):\n",
    "            result.add(b['index'])\n",
    "    return sorted(list(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = corpus.Corpus(os.path.join('moddata','nucc','data001.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = dict()\n",
    "with open(os.path.join('seeds', 'positive.txt')) as f:\n",
    "    for w in f.readlines():\n",
    "        sw = w.strip()\n",
    "        dic[sw] = corpus.WordDicElement(sw)\n",
    "        dic[sw].set_value(1.)\n",
    "with open(os.path.join('seeds', 'negative.txt')) as f:\n",
    "    for w in f.readlines():\n",
    "        sw = w.strip()\n",
    "        dic[sw] = corpus.WordDicElement(sw)\n",
    "        dic[sw].set_value(-1.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'喜ぶ': <corpus.WordDicElement at 0x1118013d0>,\n",
       " '嬉しい': <corpus.WordDicElement at 0x111ae9100>,\n",
       " '楽しい': <corpus.WordDicElement at 0x111ae97c0>,\n",
       " '美味しい': <corpus.WordDicElement at 0x111ae9d90>,\n",
       " 'きれい': <corpus.WordDicElement at 0x111ae9820>,\n",
       " '美しい': <corpus.WordDicElement at 0x1114e0dc0>,\n",
       " '楽しむ': <corpus.WordDicElement at 0x1114e03d0>,\n",
       " '悲しい': <corpus.WordDicElement at 0x111ae98e0>,\n",
       " 'さみしい': <corpus.WordDicElement at 0x111ae9e50>,\n",
       " '辛い': <corpus.WordDicElement at 0x111ae9a30>,\n",
       " '寂しい': <corpus.WordDicElement at 0x111ae9850>,\n",
       " 'きたない': <corpus.WordDicElement at 0x111ae98b0>,\n",
       " '痛い': <corpus.WordDicElement at 0x111ad49d0>,\n",
       " 'まずい': <corpus.WordDicElement at 0x111ad4400>,\n",
       " '残念': <corpus.WordDicElement at 0x111ad44f0>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<corpus.WordDicElement at 0x1118013d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic['喜ぶ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_score(cp, argdic):\n",
    "    dic = argdic.copy()\n",
    "    linecount = 0\n",
    "    for line in cp.conversation:\n",
    "        linecount += 1\n",
    "        print(f'  {linecount}')\n",
    "        for bindex, b in enumerate(line.bunsetsu_index_list):\n",
    "            bunsetsu_head = line.sentence_word_list[b]\n",
    "            try:\n",
    "                dic[bunsetsu_head.lemma].accesscount += 1\n",
    "                if(dic[bunsetsu_head.lemma].isvisited and abs(dic[bunsetsu_head.lemma].value) > 0.7):\n",
    "                    candidates = [line.bunsetsu_index_list[i] for i in line.search_candidate(bindex)]\n",
    "                    for candidate_index in candidates:\n",
    "                        candidate = line.sentence_word_list[candidate_index]\n",
    "                        try:\n",
    "                            dic[candidate.lemma].add_score(corpus.extract(dic[bunsetsu_head.lemma], dic[candidate.lemma]))\n",
    "                        except KeyError:\n",
    "                            newword = corpus.WordDicElement(candidate.lemma)\n",
    "                            newword.add_score(corpus.extract(dic[bunsetsu_head.lemma], newword))\n",
    "                            dic[candidate.lemma] = newword\n",
    "            except KeyError:\n",
    "                continue\n",
    "    return dic\n",
    "\n",
    "def calc_value(argdic, alpha=0.5):\n",
    "    dic = argdic.copy()\n",
    "    for key in dic.keys():\n",
    "        elem = dic[key]\n",
    "        if(len(elem.score) < 1):\n",
    "            continue\n",
    "        elif(elem.isvisited):\n",
    "            dic[key].set_value(sum(elem.score)/len(elem.score) * alpha + elem.value * (1-alpha))\n",
    "        else:\n",
    "            dic[key].set_value(sum(elem.score)/len(elem.score))\n",
    "        dic[key].reset_score()\n",
    "            \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef calc_score(cp, argdic):\\n    c = CaboCha.Parser()\\n    dic = argdic.copy()\\n    for line in cp.conversation:\\n        tree = c.parse(line['content'])\\n        blist = bunsetsu_list(tree)\\n        for bindex, b in enumerate(blist):\\n            try:\\n                dic[b['lemma']].accesscount += 1\\n                if(dic[b['lemma']].isvisited and abs(dic[b['lemma']].value) > 0.7):\\n                    candidates = search_candidate(bindex, blist)\\n                    for candidate_index in candidates:\\n                        candidate = blist[candidate_index]\\n                        try:\\n                            dic[candidate['lemma']].add_score(corpus.extract(dic[b['lemma']], dic[candidate['lemma']]))\\n                        except KeyError:\\n                            newword = corpus.WordDicElement(candidate.lemma)\\n                            newword.add_score(corpus.extract(dic[b['lemma']], newword))\\n                            dic[candidate['lemma']] = newword\\n            except KeyError:\\n                continue\\n    return dic\\n\\ndef calc_value(argdic, alpha=0.5):\\n    dic = argdic.copy()\\n    for key in dic.keys():\\n        elem = dic[key]\\n        if(len(elem.score) < 1):\\n            continue\\n        elif(elem.isvisited):\\n            dic[key].set_value(sum(elem.score)/len(elem.score) * alpha + elem.value * (1-alpha))\\n        else:\\n            dic[key].set_value(sum(elem.score)/len(elem.score))\\n        dic[key].reset_score()\\n            \\n    return dic\\n        \\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#旧版\n",
    "'''\n",
    "def calc_score(cp, argdic):\n",
    "    c = CaboCha.Parser()\n",
    "    dic = argdic.copy()\n",
    "    for line in cp.conversation:\n",
    "        tree = c.parse(line['content'])\n",
    "        blist = bunsetsu_list(tree)\n",
    "        for bindex, b in enumerate(blist):\n",
    "            try:\n",
    "                dic[b['lemma']].accesscount += 1\n",
    "                if(dic[b['lemma']].isvisited and abs(dic[b['lemma']].value) > 0.7):\n",
    "                    candidates = search_candidate(bindex, blist)\n",
    "                    for candidate_index in candidates:\n",
    "                        candidate = blist[candidate_index]\n",
    "                        try:\n",
    "                            dic[candidate['lemma']].add_score(corpus.extract(dic[b['lemma']], dic[candidate['lemma']]))\n",
    "                        except KeyError:\n",
    "                            newword = corpus.WordDicElement(candidate.lemma)\n",
    "                            newword.add_score(corpus.extract(dic[b['lemma']], newword))\n",
    "                            dic[candidate['lemma']] = newword\n",
    "            except KeyError:\n",
    "                continue\n",
    "    return dic\n",
    "\n",
    "def calc_value(argdic, alpha=0.5):\n",
    "    dic = argdic.copy()\n",
    "    for key in dic.keys():\n",
    "        elem = dic[key]\n",
    "        if(len(elem.score) < 1):\n",
    "            continue\n",
    "        elif(elem.isvisited):\n",
    "            dic[key].set_value(sum(elem.score)/len(elem.score) * alpha + elem.value * (1-alpha))\n",
    "        else:\n",
    "            dic[key].set_value(sum(elem.score)/len(elem.score))\n",
    "        dic[key].reset_score()\n",
    "            \n",
    "    return dic\n",
    "        \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "  1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Sentence' object has no attribute 'sentence_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(filecount)\n\u001b[1;32m      5\u001b[0m     cp \u001b[38;5;241m=\u001b[39m corpus\u001b[38;5;241m.\u001b[39mCorpus(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmoddata\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnucc\u001b[39m\u001b[38;5;124m'\u001b[39m,file))\n\u001b[0;32m----> 6\u001b[0m     dic \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m dic \u001b[38;5;241m=\u001b[39m calc_value(dic)\n",
      "Cell \u001b[0;32mIn [10], line 8\u001b[0m, in \u001b[0;36mcalc_score\u001b[0;34m(cp, argdic)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlinecount\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bindex, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(line\u001b[38;5;241m.\u001b[39mbunsetsu_index_list):\n\u001b[0;32m----> 8\u001b[0m     bunsetsu_head \u001b[38;5;241m=\u001b[39m \u001b[43mline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentence_list\u001b[49m[b]\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m         dic[bunsetsu_head\u001b[38;5;241m.\u001b[39mlemma]\u001b[38;5;241m.\u001b[39maccesscount \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sentence' object has no attribute 'sentence_list'"
     ]
    }
   ],
   "source": [
    "filecount = 0\n",
    "for file in sorted(os.listdir(os.path.join('moddata','nucc'))):    \n",
    "    filecount += 1\n",
    "    print(filecount)\n",
    "    cp = corpus.Corpus(os.path.join('moddata','nucc',file))\n",
    "    dic = calc_score(cp, dic)\n",
    "dic = calc_value(dic)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for i in dic.keys():\n",
    "    if(-0.6 < dic[i].value < 0.6 or dic[i].accesscount > 1000):\n",
    "        dic[i].set_value(0.)\n",
    "        dic[i].deactivate()\n",
    "        continue\n",
    "    output.append(str(dic[i]))\n",
    "with open('outputclassedfix1.txt', mode='w') as f:\n",
    "    f.write('\\n'.join(sorted(output, key=lambda x: int(x.split('(')[-1].replace(')','')), reverse=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a0fd0f0d99e5d6864c7c7afe1c6989aa5d650cea056a3048f58a96658917535"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
